{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb45e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d32cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8992dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 138.92 MiB, increment: 0.04 MiB\n"
     ]
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 67108864 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[0;32m    597\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 599\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[38;5;241m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[38;5;241m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[38;5;241m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\optimization.py:990\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys):\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m args, got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys), \u001b[38;5;28mlen\u001b[39m(args)))\n\u001b[1;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\core.py:149\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, out, cache)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m toposort(dsk):\n\u001b[0;32m    148\u001b[0m     task \u001b[38;5;241m=\u001b[39m dsk[key]\n\u001b[1;32m--> 149\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     cache[key] \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:97\u001b[0m, in \u001b[0;36mParquetFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(part, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     95\u001b[0m     part \u001b[38;5;241m=\u001b[39m [part]\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_parquet_part\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Temporary workaround for HLG serialization bug\u001b[39;49;00m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# (see: https://github.com/dask/dask/issues/8581)\u001b[39;49;00m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpiece\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpiece\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:644\u001b[0m, in \u001b[0;36mread_parquet_part\u001b[1;34m(fs, engine, meta, part, columns, index, use_nullable_dtypes, kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(part) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m part[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_multi_support(engine):\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;66;03m# Part kwargs expected\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     func \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mread_partition\n\u001b[1;32m--> 644\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m         func(\n\u001b[0;32m    646\u001b[0m             fs,\n\u001b[0;32m    647\u001b[0m             rg,\n\u001b[0;32m    648\u001b[0m             columns\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[0;32m    649\u001b[0m             index,\n\u001b[0;32m    650\u001b[0m             use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtoolz\u001b[38;5;241m.\u001b[39mmerge(kwargs, kw),\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (rg, kw) \u001b[38;5;129;01min\u001b[39;00m part\n\u001b[0;32m    654\u001b[0m     ]\n\u001b[0;32m    655\u001b[0m     df \u001b[38;5;241m=\u001b[39m concat(dfs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dfs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# list of parts at once\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:645\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(part) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m part[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_multi_support(engine):\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;66;03m# Part kwargs expected\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     func \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mread_partition\n\u001b[0;32m    644\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 645\u001b[0m         func(\n\u001b[0;32m    646\u001b[0m             fs,\n\u001b[0;32m    647\u001b[0m             rg,\n\u001b[0;32m    648\u001b[0m             columns\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[0;32m    649\u001b[0m             index,\n\u001b[0;32m    650\u001b[0m             use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtoolz\u001b[38;5;241m.\u001b[39mmerge(kwargs, kw),\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (rg, kw) \u001b[38;5;129;01min\u001b[39;00m part\n\u001b[0;32m    654\u001b[0m     ]\n\u001b[0;32m    655\u001b[0m     df \u001b[38;5;241m=\u001b[39m concat(dfs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dfs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# list of parts at once\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:522\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_partition\u001b[1;34m(cls, fs, pieces, columns, index, use_nullable_dtypes, categories, partitions, filters, schema, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m     row_group \u001b[38;5;241m=\u001b[39m [row_group]\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# Read in arrow table and convert to pandas\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m arrow_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_read_table(\n\u001b[0;32m    523\u001b[0m     path_or_frag,\n\u001b[0;32m    524\u001b[0m     fs,\n\u001b[0;32m    525\u001b[0m     row_group,\n\u001b[0;32m    526\u001b[0m     columns,\n\u001b[0;32m    527\u001b[0m     schema,\n\u001b[0;32m    528\u001b[0m     filters,\n\u001b[0;32m    529\u001b[0m     partitions,\n\u001b[0;32m    530\u001b[0m     partition_keys,\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    532\u001b[0m )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_read:\n\u001b[0;32m    534\u001b[0m     tables\u001b[38;5;241m.\u001b[39mappend(arrow_table)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:1614\u001b[0m, in \u001b[0;36mArrowDatasetEngine._read_table\u001b[1;34m(cls, path_or_frag, fs, row_groups, columns, schema, filters, partitions, partition_keys, **kwargs)\u001b[0m\n\u001b[0;32m   1607\u001b[0m     arrow_table \u001b[38;5;241m=\u001b[39m frag\u001b[38;5;241m.\u001b[39mto_table(\n\u001b[0;32m   1608\u001b[0m         use_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1609\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m   1610\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcols,\n\u001b[0;32m   1611\u001b[0m         \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mpq\u001b[38;5;241m.\u001b[39m_filters_to_expression(filters) \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1612\u001b[0m     )\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1614\u001b[0m     arrow_table \u001b[38;5;241m=\u001b[39m _read_table_from_path(\n\u001b[0;32m   1615\u001b[0m         path_or_frag,\n\u001b[0;32m   1616\u001b[0m         fs,\n\u001b[0;32m   1617\u001b[0m         row_groups,\n\u001b[0;32m   1618\u001b[0m         columns,\n\u001b[0;32m   1619\u001b[0m         schema,\n\u001b[0;32m   1620\u001b[0m         filters,\n\u001b[0;32m   1621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1622\u001b[0m     )\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;66;03m# For pyarrow.dataset api, if we did not read directly from\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# fragments, we need to add the partitioned columns here.\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partitions \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partitions, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:260\u001b[0m, in \u001b[0;36m_read_table_from_path\u001b[1;34m(path, fs, row_groups, columns, schema, filters, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_input_files(\n\u001b[0;32m    254\u001b[0m     [path],\n\u001b[0;32m    255\u001b[0m     fs\u001b[38;5;241m=\u001b[39mfs,\n\u001b[0;32m    256\u001b[0m     precache_options\u001b[38;5;241m=\u001b[39mprecache_options,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_file_options,\n\u001b[0;32m    258\u001b[0m )[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mas\u001b[39;00m fil:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row_groups \u001b[38;5;241m==\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pq\u001b[38;5;241m.\u001b[39mParquetFile(fil, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpre_buffer)\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    261\u001b[0m             columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    262\u001b[0m             use_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    263\u001b[0m             use_pandas_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    264\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mread_kwargs,\n\u001b[0;32m    265\u001b[0m         )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pq\u001b[38;5;241m.\u001b[39mParquetFile(fil, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpre_buffer)\u001b[38;5;241m.\u001b[39mread_row_groups(\n\u001b[0;32m    268\u001b[0m             row_groups,\n\u001b[0;32m    269\u001b[0m             columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mread_kwargs,\n\u001b[0;32m    273\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\pyarrow\\parquet\\core.py:628\u001b[0m, in \u001b[0;36mParquetFile.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;124;03mRead a Table from Parquet format.\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;124;03manimal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    626\u001b[0m column_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_column_indices(\n\u001b[0;32m    627\u001b[0m     columns, use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n\u001b[1;32m--> 628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\pyarrow\\_parquet.pyx:1375\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.read_all\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 67108864 failed"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "df = dd.read_parquet(r\"C:\\Users\\prath\\OneDrive\\Desktop\\microsoft_malware_prediction\\Data Exploration\\train.parquet\")\n",
    "df = df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d0bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 438.52 MiB, increment: 0.70 MiB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arrays chunk sizes are unknown: (nan,)\n\nA possible solution: https://docs.dask.org/en/latest/array-chunks.html#unknown-chunks\nSummary: to compute chunks sizes, use\n\n   x.compute_chunk_sizes()  # for Dask Array `x`\n   ddf.to_dask_array(lengths=True)  # for Dask DataFrame `ddf`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\array\\core.py:1993\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m   1990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1992\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetitem-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tokenize(\u001b[38;5;28mself\u001b[39m, index2)\n\u001b[1;32m-> 1993\u001b[0m dsk, chunks \u001b[38;5;241m=\u001b[39m \u001b[43mslice_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitemsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1995\u001b[0m graph \u001b[38;5;241m=\u001b[39m HighLevelGraph\u001b[38;5;241m.\u001b[39mfrom_collections(out, dsk, dependencies\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m])\n\u001b[0;32m   1997\u001b[0m meta \u001b[38;5;241m=\u001b[39m meta_from_array(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(chunks))\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\array\\slicing.py:174\u001b[0m, in \u001b[0;36mslice_array\u001b[1;34m(out_name, in_name, blockdims, index, itemsize)\u001b[0m\n\u001b[0;32m    171\u001b[0m index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m missing\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Pass down to next function\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m dsk_out, bd_out \u001b[38;5;241m=\u001b[39m \u001b[43mslice_with_newaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitemsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m bd_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m, bd_out))\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dsk_out, bd_out\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\array\\slicing.py:196\u001b[0m, in \u001b[0;36mslice_with_newaxes\u001b[1;34m(out_name, in_name, blockdims, index, itemsize)\u001b[0m\n\u001b[0;32m    193\u001b[0m         where_none[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Pass down and do work\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m dsk, blockdims2 \u001b[38;5;241m=\u001b[39m \u001b[43mslice_wrap_lists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitemsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m where_none:\n\u001b[0;32m    199\u001b[0m     expand \u001b[38;5;241m=\u001b[39m expander(where_none)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\array\\slicing.py:252\u001b[0m, in \u001b[0;36mslice_wrap_lists\u001b[1;34m(out_name, in_name, blockdims, index, itemsize)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# No lists, hooray! just use slice_slices_and_integers\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m where_list:\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mslice_slices_and_integers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Replace all lists with full slices  [3, 1, 0] -> slice(None, None, None)\u001b[39;00m\n\u001b[0;32m    255\u001b[0m index_without_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m is_arraylike(i) \u001b[38;5;28;01melse\u001b[39;00m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m index\n\u001b[0;32m    257\u001b[0m )\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\array\\slicing.py:301\u001b[0m, in \u001b[0;36mslice_slices_and_integers\u001b[1;34m(out_name, in_name, blockdims, index)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim, ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(shape, index):\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(dim) \u001b[38;5;129;01mand\u001b[39;00m ind \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrays chunk sizes are unknown: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00munknown_chunk_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         )\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(ind, (\u001b[38;5;28mslice\u001b[39m, Integral)) \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m index)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(index) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blockdims)\n",
      "\u001b[1;31mValueError\u001b[0m: Arrays chunk sizes are unknown: (nan,)\n\nA possible solution: https://docs.dask.org/en/latest/array-chunks.html#unknown-chunks\nSummary: to compute chunks sizes, use\n\n   x.compute_chunk_sizes()  # for Dask Array `x`\n   ddf.to_dask_array(lengths=True)  # for Dask DataFrame `ddf`"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "stats = []\n",
    "for col in df.columns:\n",
    "    stats.append((col, df[col].nunique(), df[col].isnull().sum() * 100 / df.shape[0], df[col].value_counts(normalize=True, dropna=False).values[0] * 100, df[col].dtype))\n",
    "\n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\n",
    "stats_df.sort_values('Percentage of missing values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4d0885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 361.19 MiB, increment: 0.00 MiB\n"
     ]
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 33554432 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m train\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m good_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (abc\u001b[38;5;241m.\u001b[39mSequence, ExtensionArray)):\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;66;03m# GH#44616 big perf improvement for e.g. pytorch tensor\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    738\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\core.py:501\u001b[0m, in \u001b[0;36m_Frame.__array__\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed)\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[0;32m    597\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 599\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[38;5;241m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[38;5;241m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[38;5;241m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\optimization.py:990\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys):\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m args, got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys), \u001b[38;5;28mlen\u001b[39m(args)))\n\u001b[1;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\core.py:149\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, out, cache)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m toposort(dsk):\n\u001b[0;32m    148\u001b[0m     task \u001b[38;5;241m=\u001b[39m dsk[key]\n\u001b[1;32m--> 149\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     cache[key] \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:97\u001b[0m, in \u001b[0;36mParquetFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(part, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     95\u001b[0m     part \u001b[38;5;241m=\u001b[39m [part]\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_parquet_part\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Temporary workaround for HLG serialization bug\u001b[39;49;00m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# (see: https://github.com/dask/dask/issues/8581)\u001b[39;49;00m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpiece\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpiece\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:644\u001b[0m, in \u001b[0;36mread_parquet_part\u001b[1;34m(fs, engine, meta, part, columns, index, use_nullable_dtypes, kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(part) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m part[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_multi_support(engine):\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;66;03m# Part kwargs expected\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     func \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mread_partition\n\u001b[1;32m--> 644\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m         func(\n\u001b[0;32m    646\u001b[0m             fs,\n\u001b[0;32m    647\u001b[0m             rg,\n\u001b[0;32m    648\u001b[0m             columns\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[0;32m    649\u001b[0m             index,\n\u001b[0;32m    650\u001b[0m             use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtoolz\u001b[38;5;241m.\u001b[39mmerge(kwargs, kw),\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (rg, kw) \u001b[38;5;129;01min\u001b[39;00m part\n\u001b[0;32m    654\u001b[0m     ]\n\u001b[0;32m    655\u001b[0m     df \u001b[38;5;241m=\u001b[39m concat(dfs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dfs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# list of parts at once\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:645\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(part) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m part[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_multi_support(engine):\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;66;03m# Part kwargs expected\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     func \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mread_partition\n\u001b[0;32m    644\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 645\u001b[0m         func(\n\u001b[0;32m    646\u001b[0m             fs,\n\u001b[0;32m    647\u001b[0m             rg,\n\u001b[0;32m    648\u001b[0m             columns\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[0;32m    649\u001b[0m             index,\n\u001b[0;32m    650\u001b[0m             use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtoolz\u001b[38;5;241m.\u001b[39mmerge(kwargs, kw),\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (rg, kw) \u001b[38;5;129;01min\u001b[39;00m part\n\u001b[0;32m    654\u001b[0m     ]\n\u001b[0;32m    655\u001b[0m     df \u001b[38;5;241m=\u001b[39m concat(dfs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dfs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# list of parts at once\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:522\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_partition\u001b[1;34m(cls, fs, pieces, columns, index, use_nullable_dtypes, categories, partitions, filters, schema, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m     row_group \u001b[38;5;241m=\u001b[39m [row_group]\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# Read in arrow table and convert to pandas\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m arrow_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_read_table(\n\u001b[0;32m    523\u001b[0m     path_or_frag,\n\u001b[0;32m    524\u001b[0m     fs,\n\u001b[0;32m    525\u001b[0m     row_group,\n\u001b[0;32m    526\u001b[0m     columns,\n\u001b[0;32m    527\u001b[0m     schema,\n\u001b[0;32m    528\u001b[0m     filters,\n\u001b[0;32m    529\u001b[0m     partitions,\n\u001b[0;32m    530\u001b[0m     partition_keys,\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    532\u001b[0m )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_read:\n\u001b[0;32m    534\u001b[0m     tables\u001b[38;5;241m.\u001b[39mappend(arrow_table)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:1614\u001b[0m, in \u001b[0;36mArrowDatasetEngine._read_table\u001b[1;34m(cls, path_or_frag, fs, row_groups, columns, schema, filters, partitions, partition_keys, **kwargs)\u001b[0m\n\u001b[0;32m   1607\u001b[0m     arrow_table \u001b[38;5;241m=\u001b[39m frag\u001b[38;5;241m.\u001b[39mto_table(\n\u001b[0;32m   1608\u001b[0m         use_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1609\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m   1610\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcols,\n\u001b[0;32m   1611\u001b[0m         \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mpq\u001b[38;5;241m.\u001b[39m_filters_to_expression(filters) \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1612\u001b[0m     )\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1614\u001b[0m     arrow_table \u001b[38;5;241m=\u001b[39m _read_table_from_path(\n\u001b[0;32m   1615\u001b[0m         path_or_frag,\n\u001b[0;32m   1616\u001b[0m         fs,\n\u001b[0;32m   1617\u001b[0m         row_groups,\n\u001b[0;32m   1618\u001b[0m         columns,\n\u001b[0;32m   1619\u001b[0m         schema,\n\u001b[0;32m   1620\u001b[0m         filters,\n\u001b[0;32m   1621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1622\u001b[0m     )\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;66;03m# For pyarrow.dataset api, if we did not read directly from\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# fragments, we need to add the partitioned columns here.\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partitions \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partitions, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:260\u001b[0m, in \u001b[0;36m_read_table_from_path\u001b[1;34m(path, fs, row_groups, columns, schema, filters, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_input_files(\n\u001b[0;32m    254\u001b[0m     [path],\n\u001b[0;32m    255\u001b[0m     fs\u001b[38;5;241m=\u001b[39mfs,\n\u001b[0;32m    256\u001b[0m     precache_options\u001b[38;5;241m=\u001b[39mprecache_options,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_file_options,\n\u001b[0;32m    258\u001b[0m )[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mas\u001b[39;00m fil:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row_groups \u001b[38;5;241m==\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pq\u001b[38;5;241m.\u001b[39mParquetFile(fil, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpre_buffer)\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    261\u001b[0m             columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    262\u001b[0m             use_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    263\u001b[0m             use_pandas_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    264\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mread_kwargs,\n\u001b[0;32m    265\u001b[0m         )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pq\u001b[38;5;241m.\u001b[39mParquetFile(fil, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpre_buffer)\u001b[38;5;241m.\u001b[39mread_row_groups(\n\u001b[0;32m    268\u001b[0m             row_groups,\n\u001b[0;32m    269\u001b[0m             columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mread_kwargs,\n\u001b[0;32m    273\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\pyarrow\\parquet\\core.py:628\u001b[0m, in \u001b[0;36mParquetFile.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;124;03mRead a Table from Parquet format.\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;124;03manimal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    626\u001b[0m column_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_column_indices(\n\u001b[0;32m    627\u001b[0m     columns, use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n\u001b[1;32m--> 628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\pyarrow\\_parquet.pyx:1375\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.read_all\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 33554432 failed"
     ]
    }
   ],
   "source": [
    "%memit\n",
    "train= pd.DataFrame(df)\n",
    "good_cols = list(df.columns)\n",
    "for col in df.columns:\n",
    "    rate = df[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate > 0.9: # Removing Missing Values if \n",
    "        good_cols.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea58848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "train= df[good_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "def plot_categorical_feature(col, only_bars=False, top_n=10, by_touch=False):\n",
    "    top_n = top_n if train[col].nunique() > top_n else train[col].nunique()\n",
    "    print(f\"{col} has {train[col].nunique()} unique values and type: {train[col].dtype}.\")\n",
    "    print(train[col].value_counts(normalize=True, dropna=False).head())\n",
    "    if not by_touch:\n",
    "        if not only_bars:\n",
    "            df = train.groupby([col]).agg({'HasDetections': ['count', 'mean']})\n",
    "            df = df.sort_values(('HasDetections', 'count'), ascending=False).head(top_n).sort_index()\n",
    "            fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "            fig.add_trace(go.Bar(x=df.index, y=df['HasDetections']['count'].values, name='counts'), secondary_y=False)\n",
    "            fig.add_trace(go.Scatter(x=df.index, y=df['HasDetections']['mean'], name='Detections rate'), secondary_y=True)\n",
    "            fig.update_layout(title=f\"Counts of {col} by top-{top_n} categories and mean target value\",\n",
    "                              xaxis=dict(title=f'{col}',\n",
    "                                         showgrid=False,\n",
    "                                         zeroline=False,\n",
    "                                         showline=False,),\n",
    "                              yaxis=dict(title='Counts',\n",
    "                                         showgrid=False,\n",
    "                                         zeroline=False,\n",
    "                                         showline=False,),\n",
    "                              yaxis2=dict(title='Detections rate', overlaying='y', side='right'),\n",
    "                              legend=dict(orientation=\"v\"))\n",
    "        else:\n",
    "            top_cat = list(train[col].value_counts(dropna=False).index[:top_n])\n",
    "            df0 = train.loc[(train[col].isin(top_cat)) & (train['HasDetections'] == 1), col].value_counts().head(10).sort_index()\n",
    "            df1 = train.loc[(train[col].isin(top_cat)) & (train['HasDetections'] == 0), col].value_counts().head(10).sort_index()\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Bar(x=df0.index, y=df0.values, name='Has Detections'))\n",
    "            fig.add_trace(go.Bar(x=df1.index, y=df1.values, name='No Detections'))\n",
    "            fig.update_layout(title=f\"Counts of {col} by top-{top_n} categories\",\n",
    "                              xaxis=dict(title=f'{col}',\n",
    "                                         showgrid=False,\n",
    "                                         zeroline=False,\n",
    "                                         showline=False,),\n",
    "                              yaxis=dict(title='Counts',\n",
    "                                         showgrid=False,\n",
    "                                         zeroline=False,\n",
    "                                         showline=False,),\n",
    "                              legend=dict(orientation=\"v\"), barmode='group')\n",
    "\n",
    "        fig.show()\n",
    "    else:\n",
    "        top_n = 10\n",
    "        top_cat = list(train[col].value_counts(dropna=False).index[:top_n])\n",
    "        df = train.loc[train[col].isin(top_cat)]\n",
    "\n",
    "        df1 = train.loc[train['Census_IsTouchEnabled'] == 1]\n",
    "        df0 = train.loc[train['Census_IsTouchEnabled'] == 0]\n",
    "\n",
    "        df0_ = df0.groupby([col]).agg({'HasDetections': ['count', 'mean']})\n",
    "        df0_ = df0_.sort_values(('HasDetections', 'count'), ascending=False).head(top_n).sort_index()\n",
    "        df1_ = df1.groupby([col]).agg({'HasDetections': ['count', 'mean']})\n",
    "        df1_ = df1_.loc[df0_.index]\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=df0_[('HasDetections', 'mean')], y=df0_.index,\n",
    "            orientation='h', name='Census_IsTouchEnabled=0', marker=dict(color='royalblue')\n",
    "        ))\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=df1_[('HasDetections', 'mean')], y=df1_.index,\n",
    "            orientation='h', name='Census_IsTouchEnabled=1', marker=dict(color='seagreen')\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title=f'Distribution of HasDetections by {col}',\n",
    "            xaxis_title='% of HasDetections',\n",
    "            yaxis_title=col,\n",
    "            legend=dict(x=0.85, y=0.95),\n",
    "            margin=dict(l=120, r=20, t=50, b=50),\n",
    "            height=600\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_InternalPrimaryDisplayResolutionHorizontal', True,by_touch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_InternalPrimaryDisplayResolutionVertical', True,by_touch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_InternalBatteryNumberOfCharges', True,by_touch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_OSVersion', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05297ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_OSBranch', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_OSBuildRevision', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420876da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_FirmwareManufacturerIdentifier', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('OsBuild', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_ChassisTypeName', True,by_touch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_InternalBatteryType', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ef598",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_OSEdition', True,by_touch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b44529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_OSSkuName', True,by_touch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_OSInstallLanguageIdentifier', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_OSUILocaleIdentifier', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e71701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('OsSuite', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "fig = px.histogram(train, x=\"SmartScreen\", color=\"HasDetections\",\n",
    "                   barmode=\"group\", nbins=len(train['SmartScreen'].unique()))\n",
    "\n",
    "# Update the layout with title and axis labels\n",
    "fig.update_layout(title=\"SmartScreen counts\", xaxis_title=\"SmartScreen\",\n",
    "                  yaxis_title=\"Count\")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab199e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature('Census_MDC2FormFactor', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "fig = px.histogram(train, x=\"Census_PrimaryDiskTypeName\", color=\"HasDetections\",facet_col_wrap=3,\n",
    "                   facet_col=\"Census_MDC2FormFactor\", barmode=\"group\",\n",
    "                   nbins=len(train['Census_PrimaryDiskTypeName'].unique()))\n",
    "\n",
    "# Update the layout with title and axis labels\n",
    "fig.update_layout(title=\"Census PrimaryDiskTypeName counts by Census_MDC2FormFactor\",\n",
    "                  xaxis_title=\"Census_PrimaryDiskTypeName\", yaxis_title=\"Count\",height=1000,width=1000)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature(\"Census_ProcessorManufacturerIdentifier\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature(\"Census_PowerPlatformRoleName\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit\n",
    "plot_categorical_feature(\"Census_OSInstallTypeName\",True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
